{
  "cells": [
    {
      "cell_type": "code",
      "id": "5F59tKKJ1qcUdEL2ZS9riPMv",
      "metadata": {
        "tags": [],
        "id": "5F59tKKJ1qcUdEL2ZS9riPMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e89a78-c421-4634-c646-e683e10678e9"
      },
      "source": [
        "!pip install google-cloud-aiplatform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.122.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.28.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (6.33.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.46.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.11.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt evaluation for Gen AI summaries"
      ],
      "metadata": {
        "id": "rSWDTUxCK4p4"
      },
      "id": "rSWDTUxCK4p4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Initial Evaluation: The Hallucination Score (List-Wise):\n",
        "This code block establishes a crucial baseline: testing the factual accuracy or fidelity of the generated summaries against their source text.\n",
        "\n"
      ],
      "metadata": {
        "id": "7LxtOoOV_5o1"
      },
      "id": "7LxtOoOV_5o1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Problem it Solved:\n",
        "The core goal was to detect hallucinations‚Äîinstances where the Large Language Model (LLM) generated information that was not present or was contradictory to the provided source text. Hallucinations are the most severe failure mode for a summarization task, so testing for them first is essential.\n",
        "\n",
        "We also wished to ranking and differentiate high-quality summaries. We needed a metric that was hard to max out, providing enough signal to distinguish between a perfect, wordy summary and a perfect, concise one.\n",
        "\n",
        "## How it Works:\n",
        "\n",
        "**Comparative Judging**: For each source text, all three summaries were generated and then sent to the Judge model simultaneously. This forced the Judge to look at the summaries side-by-side and rank them against each other.\n",
        "\n",
        "**Generator Model**: Used the gemini-2.0-flash model (the Generator) to produce summaries based on three different prompt styles.\n",
        "\n",
        "**List-Wise Evaluation**: By sending the source text and all generated summary variations to the Judge model in a single prompt, we force the model to rank them against each other and notice subtle differences.\n",
        "\n",
        "**Judge Model & Metric**: Used a strong model like gemini-2.5-pro (the Judge) and a scoring scale where 1.0 was perfect (no hallucination) and 10.0 was a complete fabrication, and allowing decimal scoring for robustness."
      ],
      "metadata": {
        "id": "i_e83CG8LIMl"
      },
      "id": "i_e83CG8LIMl"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-cloud-aiplatform\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Initialize Vertex AI\n",
        "# ==========================================\n",
        "PROJECT_ID = \"pulseai-team3-ba882-fall25\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION: Model Definitions\n",
        "# ==========================================\n",
        "# Ensure these IDs are available in your specific GCP project\n",
        "GENERATOR_MODEL_ID = \"gemini-2.0-flash\"\n",
        "JUDGE_MODEL_ID = \"gemini-2.5-pro\" # Suggest changing to 1.5-pro or a valid ID\n",
        "\n",
        "print(f\"üöÄ Initialized Vertex AI project {PROJECT_ID}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. CORE FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def generate_summary(prompt_template, source_text):\n",
        "    \"\"\"Generates the summary using the Flash model.\"\"\"\n",
        "    model = GenerativeModel(GENERATOR_MODEL_ID)\n",
        "    config = GenerationConfig(temperature=0.4, max_output_tokens=1024)\n",
        "\n",
        "    try:\n",
        "        final_prompt = prompt_template.format(text=source_text)\n",
        "        response = model.generate_content(final_prompt, generation_config=config)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"[Error]: {str(e)}\"\n",
        "\n",
        "def evaluate_comparative_hallucination(source_text, summaries_dict):\n",
        "    \"\"\"\n",
        "    Judges multiple summaries side-by-side to force differentiation.\n",
        "\n",
        "    Args:\n",
        "        source_text (str): The original source.\n",
        "        summaries_dict (dict): { \"prompt_id\": \"summary text\", ... }\n",
        "    \"\"\"\n",
        "    model = GenerativeModel(JUDGE_MODEL_ID)\n",
        "\n",
        "    # Construct a string representation of all candidates\n",
        "    candidates_str = \"\"\n",
        "    for pid, summary in summaries_dict.items():\n",
        "        candidates_str += f\"\\n--- CANDIDATE ID: {pid} ---\\n{summary}\\n\"\n",
        "\n",
        "    judge_prompt = f\"\"\"\n",
        "    You are a strict Factual Consistency Judge.\n",
        "    You will be provided with a [SOURCE] text and multiple [CANDIDATE SUMMARIES].\n",
        "\n",
        "    Task:\n",
        "    1. Compare the candidates against the source AND against each other.\n",
        "    2. Look for \"hallucinations\" (facts not in source) and \"omissions\" (missing critical context that alters meaning).\n",
        "    3. Assign a \"hallucination_score\" (1.0 to 10.0).\n",
        "       - 10.0: Severe fabrication.\n",
        "       - 1.0: Perfect consistency.\n",
        "    4. DIFFERENTIATION IS KEY: If two summaries are good, give the one that captures more nuance a better (lower) score.\n",
        "       If they are identical in quality, give them the same score.\n",
        "\n",
        "    Output STRICT JSON format:\n",
        "    {{\n",
        "        \"scores\": {{\n",
        "            \"prompt_id_1\": {{ \"score\": float, \"reason\": \"string\" }},\n",
        "            \"prompt_id_2\": {{ \"score\": float, \"reason\": \"string\" }}\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    [SOURCE]\n",
        "    {source_text}\n",
        "\n",
        "    [CANDIDATE SUMMARIES]\n",
        "    {candidates_str}\n",
        "    \"\"\"\n",
        "\n",
        "    config = GenerationConfig(\n",
        "        temperature=0.0,\n",
        "        response_mime_type=\"application/json\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(judge_prompt, generation_config=config)\n",
        "        result = json.loads(response.text)\n",
        "        return result.get('scores', {})\n",
        "    except Exception as e:\n",
        "        print(f\"Judge Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXECUTION LOOP (RESTRUCTURED)\n",
        "# ==========================================\n",
        "test_sources = {\n",
        "    \"Source 1 (Physics)\": \"Newton's first law states that an object at rest stays at rest and an object in motion stays in motion with the same speed and in the same direction unless acted upon by an unbalanced force. Essentially, objects resist changes in their state of motion; it takes a force to start something moving or to stop it, change its speed, or alter its direction. Example: When a car stops suddenly, your body wants to keep moving forward (inertia).\",\n",
        "    \"Source 2 (History)\": \"The Apollo 11 mission landed the first humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldrin formed the American crew that landed the Apollo Lunar Module Eagle on July 20, 1969. The Apollo Guidance Computer (AGC) and Display and Keyboard (DSKY) instrument panel were created specifically for the programme. The DSKY panel was akin to a calculator keyboard and display, working with codes the astronauts learned or checked from a flight manual.\",\n",
        "    \"Source 3 (Finance)\": \"EBITDA stands for Earnings Before Interest, Taxes, Depreciation, and Amortization. It is a measure of a company's overall financial performance and is used as an alternative to net income in some circumstances. It's criticized for ignoring asset costs and potential accounting manipulation, as it's not a GAAP metric. It doesn't account for the cash needed to replace or maintain assets (capex), which are significant in asset-intensive businesses.\"\n",
        "}\n",
        "\n",
        "prompt_candidates = {\n",
        "    \"101\": \"You are an expert technical editor. Summarize this: {text}\",\n",
        "    \"102\": \"You are an editor. Summarize this: {text}\",\n",
        "    \"103\": \"Summarize this: {text}\"\n",
        "}\n",
        "\n",
        "# We need to store results in a way we can pivot later\n",
        "# Structure: { \"101\": {\"Source 1\": 1.0, \"Source 2\": ...}, \"102\": ... }\n",
        "formatted_results = {pid: {\"Prompt index\": pid} for pid in prompt_candidates}\n",
        "\n",
        "print(\"\\n‚ö° Starting Comparative Evaluation Pipeline...\")\n",
        "\n",
        "for source_name, source_text in test_sources.items():\n",
        "    print(f\"\\nProcessing {source_name}...\")\n",
        "\n",
        "    # 1. Generate ALL summaries for this source\n",
        "    current_batch = {}\n",
        "    for pid, template in prompt_candidates.items():\n",
        "        print(f\"  - Generating {pid}...\")\n",
        "        current_batch[pid] = generate_summary(template, source_text)\n",
        "        time.sleep(0.2) # Avoid rate limits\n",
        "\n",
        "    # 2. Compare them ALL in one shot\n",
        "    print(f\"  - ‚öñÔ∏è Judging comparison...\")\n",
        "    scores_dict = evaluate_comparative_hallucination(source_text, current_batch)\n",
        "\n",
        "    # 3. Store results\n",
        "    for pid in prompt_candidates:\n",
        "        # Default to 0 if scoring failed\n",
        "        score_data = scores_dict.get(pid, {\"score\": 0, \"reason\": \"Error\"})\n",
        "        formatted_results[pid][source_name] = score_data['score']\n",
        "\n",
        "# ==========================================\n",
        "# 5. OUTPUT\n",
        "# ==========================================\n",
        "# Convert dictionary of rows to list\n",
        "results_data = list(formatted_results.values())\n",
        "\n",
        "# Calculate Averages\n",
        "for row in results_data:\n",
        "    scores = [row[k] for k in test_sources.keys() if isinstance(row[k], (int, float))]\n",
        "    row[\"Average\"] = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "df = pd.DataFrame(results_data)\n",
        "cols = [\"Prompt index\"] + list(test_sources.keys()) + [\"Average\"]\n",
        "df = df[cols]\n",
        "\n",
        "print(\"\\n‚úÖ Final Scoreboard:\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(df.style.background_gradient(cmap='RdYlGn_r', vmin=1, vmax=10))\n",
        "except ImportError:\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "4CGpqXA7weBV",
        "outputId": "fb098ca9-bff3-486d-9834-9840e2049ce5"
      },
      "id": "4CGpqXA7weBV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initialized Vertex AI project pulseai-team3-ba882-fall25\n",
            "\n",
            "‚ö° Starting Comparative Evaluation Pipeline...\n",
            "\n",
            "Processing Source 1 (Physics)...\n",
            "  - Generating 101...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Generating 102...\n",
            "  - Generating 103...\n",
            "  - ‚öñÔ∏è Judging comparison...\n",
            "\n",
            "Processing Source 2 (History)...\n",
            "  - Generating 101...\n",
            "  - Generating 102...\n",
            "  - Generating 103...\n",
            "  - ‚öñÔ∏è Judging comparison...\n",
            "\n",
            "Processing Source 3 (Finance)...\n",
            "  - Generating 101...\n",
            "  - Generating 102...\n",
            "  - Generating 103...\n",
            "  - ‚öñÔ∏è Judging comparison...\n",
            "\n",
            "‚úÖ Final Scoreboard:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7d070c441070>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_7bf8e_row0_col1, #T_7bf8e_row0_col2, #T_7bf8e_row2_col2, #T_7bf8e_row2_col3 {\n",
              "  background-color: #006837;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7bf8e_row0_col3, #T_7bf8e_row1_col1, #T_7bf8e_row1_col2 {\n",
              "  background-color: #0e8245;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7bf8e_row0_col4 {\n",
              "  background-color: #04703b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7bf8e_row1_col3, #T_7bf8e_row2_col4 {\n",
              "  background-color: #219c52;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7bf8e_row1_col4 {\n",
              "  background-color: #128a49;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7bf8e_row2_col1 {\n",
              "  background-color: #b7e075;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_7bf8e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_7bf8e_level0_col0\" class=\"col_heading level0 col0\" >Prompt index</th>\n",
              "      <th id=\"T_7bf8e_level0_col1\" class=\"col_heading level0 col1\" >Source 1 (Physics)</th>\n",
              "      <th id=\"T_7bf8e_level0_col2\" class=\"col_heading level0 col2\" >Source 2 (History)</th>\n",
              "      <th id=\"T_7bf8e_level0_col3\" class=\"col_heading level0 col3\" >Source 3 (Finance)</th>\n",
              "      <th id=\"T_7bf8e_level0_col4\" class=\"col_heading level0 col4\" >Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_7bf8e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_7bf8e_row0_col0\" class=\"data row0 col0\" >101</td>\n",
              "      <td id=\"T_7bf8e_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
              "      <td id=\"T_7bf8e_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
              "      <td id=\"T_7bf8e_row0_col3\" class=\"data row0 col3\" >1.500000</td>\n",
              "      <td id=\"T_7bf8e_row0_col4\" class=\"data row0 col4\" >1.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7bf8e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_7bf8e_row1_col0\" class=\"data row1 col0\" >102</td>\n",
              "      <td id=\"T_7bf8e_row1_col1\" class=\"data row1 col1\" >1.500000</td>\n",
              "      <td id=\"T_7bf8e_row1_col2\" class=\"data row1 col2\" >1.500000</td>\n",
              "      <td id=\"T_7bf8e_row1_col3\" class=\"data row1 col3\" >2.000000</td>\n",
              "      <td id=\"T_7bf8e_row1_col4\" class=\"data row1 col4\" >1.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7bf8e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_7bf8e_row2_col0\" class=\"data row2 col0\" >103</td>\n",
              "      <td id=\"T_7bf8e_row2_col1\" class=\"data row2 col1\" >4.000000</td>\n",
              "      <td id=\"T_7bf8e_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
              "      <td id=\"T_7bf8e_row2_col3\" class=\"data row2 col3\" >1.000000</td>\n",
              "      <td id=\"T_7bf8e_row2_col4\" class=\"data row2 col4\" >2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Completeness and Conciseness score:\n",
        "\n",
        "This works in a similar way to the above code with these key differences:\n",
        "\n",
        "**Identify Key Facts**: The Judge first lists the most critical information points in the source.\n",
        "\n",
        "**Completeness Check**: It checks how many of these key facts each summary captured.\n",
        "\n",
        "**Conciseness Tie-Breaker**: It assesses the efficiency. If two summaries captured the same number of facts, the Judge gave the higher score to the shorter, less-verbose summary.\n",
        "\n",
        "**Quality Score**: The final score is now on a scale where 10.0 is the best result (high completeness and conciseness), allowing for a clearer differentiation on the final scoreboard."
      ],
      "metadata": {
        "id": "ugGUuN1k49UE"
      },
      "id": "ugGUuN1k49UE"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-cloud-aiplatform\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Initialize Vertex AI\n",
        "# ==========================================\n",
        "PROJECT_ID = \"pulseai-team3-ba882-fall25\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "GENERATOR_MODEL_ID = \"gemini-2.0-flash\"\n",
        "JUDGE_MODEL_ID = \"gemini-2.5-pro\" # Using 1.5 Pro for better reasoning capabilities\n",
        "\n",
        "# ==========================================\n",
        "# 2. CORE FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def generate_summary(prompt_template, source_text):\n",
        "    \"\"\"Generates the summary using the Flash model.\"\"\"\n",
        "    model = GenerativeModel(GENERATOR_MODEL_ID)\n",
        "    config = GenerationConfig(temperature=0.4, max_output_tokens=1024)\n",
        "\n",
        "    try:\n",
        "        final_prompt = prompt_template.format(text=source_text)\n",
        "        response = model.generate_content(final_prompt, generation_config=config)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"[Error]: {str(e)}\"\n",
        "\n",
        "def evaluate_completeness_and_conciseness(source_text, summaries_dict):\n",
        "    \"\"\"\n",
        "    Judges summaries based on how many key facts they retain (Completeness)\n",
        "    and penalizes unnecessary fluff (Conciseness).\n",
        "    \"\"\"\n",
        "    model = GenerativeModel(JUDGE_MODEL_ID)\n",
        "\n",
        "    candidates_str = \"\"\n",
        "    for pid, summary in summaries_dict.items():\n",
        "        candidates_str += f\"\\n--- CANDIDATE ID: {pid} ---\\n{summary}\\n\"\n",
        "\n",
        "    judge_prompt = f\"\"\"\n",
        "    You are an expert Content Quality Evaluator.\n",
        "\n",
        "    Goal: Rate the following summaries based on COMPLETENESS and CONCISENESS.\n",
        "\n",
        "    [SOURCE TEXT]\n",
        "    {source_text}\n",
        "\n",
        "    [CANDIDATE SUMMARIES]\n",
        "    {candidates_str}\n",
        "\n",
        "    Evaluation Steps (Perform these internally):\n",
        "    1. Analyze the [SOURCE TEXT] and identify the specific \"Key Information Points\" (facts, dates, names, core concepts).\n",
        "    2. For each Candidate, check which of these points are present.\n",
        "    3. Assess the word count/verbosity.\n",
        "\n",
        "    Scoring Rubric (1.0 to 10.0):\n",
        "    - 10.0: Perfect. Captures ALL key facts AND is very concise.\n",
        "    - 8.0 - 9.0: Captures all key facts but slightly wordy.\n",
        "    - 5.0 - 7.0: Misses 1 minor fact OR is very repetitive/verbose.\n",
        "    - 3.0 - 4.0: Misses significant facts.\n",
        "    - 1.0 - 2.0: Misses the main point entirely.\n",
        "\n",
        "    Tie-Breaker Rule: If two summaries capture the exact same facts, the shorter (more concise) one MUST receive the higher score.\n",
        "\n",
        "    Output STRICT JSON format:\n",
        "    {{\n",
        "        \"scores\": {{\n",
        "            \"prompt_id_1\": {{ \"score\": float, \"reason\": \"string\" }},\n",
        "            \"prompt_id_2\": {{ \"score\": float, \"reason\": \"string\" }}\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    config = GenerationConfig(\n",
        "        temperature=0.0,\n",
        "        response_mime_type=\"application/json\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(judge_prompt, generation_config=config)\n",
        "        result = json.loads(response.text)\n",
        "        return result.get('scores', {})\n",
        "    except Exception as e:\n",
        "        print(f\"Judge Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXECUTION LOOP\n",
        "# ==========================================\n",
        "test_sources = {\n",
        "    \"Source 1 (Physics)\": \"Newton's first law states that an object at rest stays at rest and an object in motion stays in motion with the same speed and in the same direction unless acted upon by an unbalanced force. Essentially, objects resist changes in their state of motion; it takes a force to start something moving or to stop it, change its speed, or alter its direction. Example: When a car stops suddenly, your body wants to keep moving forward (inertia).\",\n",
        "    \"Source 2 (History)\": \"The Apollo 11 mission landed the first humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldrin formed the American crew that landed the Apollo Lunar Module Eagle on July 20, 1969. The Apollo Guidance Computer (AGC) and Display and Keyboard (DSKY) instrument panel were created specifically for the programme. The DSKY panel was akin to a calculator keyboard and display, working with codes the astronauts learned or checked from a flight manual.\",\n",
        "    \"Source 3 (Finance)\": \"EBITDA stands for Earnings Before Interest, Taxes, Depreciation, and Amortization. It is a measure of a company's overall financial performance and is used as an alternative to net income in some circumstances. It's criticized for ignoring asset costs and potential accounting manipulation, as it's not a GAAP metric. It doesn't account for the cash needed to replace or maintain assets (capex), which are significant in asset-intensive businesses.\"\n",
        "}\n",
        "\n",
        "# Added a \"Short\" prompt to test if the judge actually rewards conciseness\n",
        "prompt_candidates = {\n",
        "    \"101\": \"You are an expert technical editor. Summarize this: {text}\",\n",
        "    \"102\": \"You are an editor. Summarize this: {text}\",\n",
        "    \"103\": \"Summarize this: {text}\"\n",
        "}\n",
        "# prompt_candidates = {\n",
        "#     \"101 (Verbose)\": \"You are a detailed technical editor. Summarize this comprehensively: {text}\",\n",
        "#     \"102 (Standard)\": \"Summarize this: {text}\",\n",
        "#     \"103 (Concise)\": \"Summarize this in one short sentence: {text}\"\n",
        "# }\n",
        "\n",
        "formatted_results = {pid: {\"Prompt index\": pid} for pid in prompt_candidates}\n",
        "\n",
        "print(\"\\n‚ö° Starting Quality Evaluation Pipeline...\")\n",
        "\n",
        "for source_name, source_text in test_sources.items():\n",
        "    print(f\"\\nProcessing {source_name}...\")\n",
        "\n",
        "    # 1. Generate ALL summaries\n",
        "    current_batch = {}\n",
        "    for pid, template in prompt_candidates.items():\n",
        "        # print(f\"  - Generating {pid}...\")\n",
        "        current_batch[pid] = generate_summary(template, source_text)\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    # 2. Compare them ALL\n",
        "    print(f\"  - ‚öñÔ∏è Judging Completeness & Conciseness...\")\n",
        "    scores_dict = evaluate_completeness_and_conciseness(source_text, current_batch)\n",
        "\n",
        "    # 3. Store results\n",
        "    for pid in prompt_candidates:\n",
        "        score_data = scores_dict.get(pid, {\"score\": 0, \"reason\": \"Error\"})\n",
        "        formatted_results[pid][source_name] = score_data['score']\n",
        "\n",
        "# ==========================================\n",
        "# 4. OUTPUT\n",
        "# ==========================================\n",
        "results_data = list(formatted_results.values())\n",
        "\n",
        "# Calculate Averages\n",
        "for row in results_data:\n",
        "    scores = [row[k] for k in test_sources.keys() if isinstance(row[k], (int, float))]\n",
        "    row[\"Average\"] = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "df = pd.DataFrame(results_data)\n",
        "cols = [\"Prompt index\"] + list(test_sources.keys()) + [\"Average\"]\n",
        "df = df[cols]\n",
        "\n",
        "print(\"\\n‚úÖ Final Scoreboard (Higher Score = Better Completeness/Conciseness):\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    # Using 'RdYlGn' (Red to Green) because 10 is now GOOD\n",
        "    display(df.style.background_gradient(cmap='RdYlGn', vmin=1, vmax=10))\n",
        "except ImportError:\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "NG4dipvXyLM8",
        "outputId": "532e702b-ac85-4932-b9df-d3bf7877b024"
      },
      "id": "NG4dipvXyLM8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö° Starting Quality Evaluation Pipeline...\n",
            "\n",
            "Processing Source 1 (Physics)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - ‚öñÔ∏è Judging Completeness & Conciseness...\n",
            "\n",
            "Processing Source 2 (History)...\n",
            "  - ‚öñÔ∏è Judging Completeness & Conciseness...\n",
            "\n",
            "Processing Source 3 (Finance)...\n",
            "  - ‚öñÔ∏è Judging Completeness & Conciseness...\n",
            "\n",
            "‚úÖ Final Scoreboard (Higher Score = Better Completeness/Conciseness):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7d06fe5666c0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_32b1d_row0_col1, #T_32b1d_row0_col3, #T_32b1d_row2_col2 {\n",
              "  background-color: #219c52;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_32b1d_row0_col2, #T_32b1d_row1_col3 {\n",
              "  background-color: #4bb05c;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_32b1d_row0_col4 {\n",
              "  background-color: #30a356;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_32b1d_row1_col1, #T_32b1d_row1_col2, #T_32b1d_row2_col3 {\n",
              "  background-color: #b7e075;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_32b1d_row1_col4 {\n",
              "  background-color: #98d368;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_32b1d_row2_col1, #T_32b1d_row2_col4 {\n",
              "  background-color: #73c264;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_32b1d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_32b1d_level0_col0\" class=\"col_heading level0 col0\" >Prompt index</th>\n",
              "      <th id=\"T_32b1d_level0_col1\" class=\"col_heading level0 col1\" >Source 1 (Physics)</th>\n",
              "      <th id=\"T_32b1d_level0_col2\" class=\"col_heading level0 col2\" >Source 2 (History)</th>\n",
              "      <th id=\"T_32b1d_level0_col3\" class=\"col_heading level0 col3\" >Source 3 (Finance)</th>\n",
              "      <th id=\"T_32b1d_level0_col4\" class=\"col_heading level0 col4\" >Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_32b1d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_32b1d_row0_col0\" class=\"data row0 col0\" >101</td>\n",
              "      <td id=\"T_32b1d_row0_col1\" class=\"data row0 col1\" >9.000000</td>\n",
              "      <td id=\"T_32b1d_row0_col2\" class=\"data row0 col2\" >8.500000</td>\n",
              "      <td id=\"T_32b1d_row0_col3\" class=\"data row0 col3\" >9.000000</td>\n",
              "      <td id=\"T_32b1d_row0_col4\" class=\"data row0 col4\" >8.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32b1d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_32b1d_row1_col0\" class=\"data row1 col0\" >102</td>\n",
              "      <td id=\"T_32b1d_row1_col1\" class=\"data row1 col1\" >7.000000</td>\n",
              "      <td id=\"T_32b1d_row1_col2\" class=\"data row1 col2\" >7.000000</td>\n",
              "      <td id=\"T_32b1d_row1_col3\" class=\"data row1 col3\" >8.500000</td>\n",
              "      <td id=\"T_32b1d_row1_col4\" class=\"data row1 col4\" >7.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32b1d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_32b1d_row2_col0\" class=\"data row2 col0\" >103</td>\n",
              "      <td id=\"T_32b1d_row2_col1\" class=\"data row2 col1\" >8.000000</td>\n",
              "      <td id=\"T_32b1d_row2_col2\" class=\"data row2 col2\" >9.000000</td>\n",
              "      <td id=\"T_32b1d_row2_col3\" class=\"data row2 col3\" >7.000000</td>\n",
              "      <td id=\"T_32b1d_row2_col4\" class=\"data row2 col4\" >8.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Prompt eval sdbhide"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}