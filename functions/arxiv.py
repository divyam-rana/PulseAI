# -*- coding: utf-8 -*-
"""Arxiv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GBS0cjEyulqJmS5HcOVjmhuRn_XZE9W0
"""

import arxiv
import datetime
import pandas as pd
import requests

def fetch_ai_papers_to_dataframe(days_ago=7, max_results=100):
    """
    Fetches extensive metadata for the latest AI-related papers from arXiv
    and returns it as a Pandas DataFrame.
    """
    print(f"ðŸš€ Fetching metadata for the latest {max_results} AI papers from the last {days_ago} days...")

    # Calculate the date for one week ago from today
    one_week_ago = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=days_ago)

    # Construct the search query for major AI categories
    query = "cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV"

    # Create the search object
    search = arxiv.Search(
      query=query,
      max_results=max_results,
      sort_by=arxiv.SortCriterion.SubmittedDate,
      sort_order=arxiv.SortOrder.Descending
    )

    # List to hold the metadata for each paper
    papers_data = []

    # Iterate over the search results
    for result in search.results():
        # Check if the paper was published within the last week
        if result.published >= one_week_ago:

            # Extract core arXiv ID from the entry_id URL
            arxiv_id = result.entry_id.split('/')[-1]

            # Extract author names into a simple list
            author_names = [author.name for author in result.authors]

            # Create a dictionary with all the desired metadata
            paper_info = {
                'arxiv_id': arxiv_id,
                'title': result.title,
                'authors': author_names,
                'summary': result.summary,
                'published_date': result.published,
                'updated_date': result.updated,
                'pdf_url': result.pdf_url,
                'primary_category': result.primary_category,
                'all_categories': result.categories,
                'doi': result.doi,
                'journal_ref': result.journal_ref,
                'comments': result.comment
            }
            papers_data.append(paper_info)

    if not papers_data:
        print("No new papers found in the specified categories for the last week.")
        return None

    # Create a DataFrame from the list of dictionaries
    df = pd.DataFrame(papers_data)

    print(f"\nâœ… Successfully collected metadata for {len(df)} papers.")
    return df

df = fetch_ai_papers_to_dataframe(days_ago=7, max_results=100)

