{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d5a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52ce0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ai_papers_to_dataframe(days_ago=DAYS_AGO, max_results=MAX_RESULTS):\n",
    "    \"\"\"\n",
    "    Fetches extensive metadata for the latest AI-related papers from arXiv \n",
    "    and returns it as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸš€ Fetching metadata for the latest {MAX_RESULTS} AI papers from the last {DAYS_AGO} days...\")\n",
    "    \n",
    "    # Calculate the date for one week ago from today\n",
    "    one_week_ago = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=DAYS_AGO)\n",
    "    \n",
    "    # Construct the search query for major AI categories\n",
    "    query = \"cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV\"\n",
    "    \n",
    "    # Create the search object\n",
    "    search = arxiv.Search(\n",
    "      query=query,\n",
    "      max_results=MAX_RESULTS,\n",
    "      sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "      sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # List to hold the metadata for each paper\n",
    "    papers_data = []\n",
    "    \n",
    "    # Iterate over the search results\n",
    "    for result in search.results():\n",
    "        # Check if the paper was published within the last week\n",
    "        if result.published >= one_week_ago:\n",
    "            \n",
    "            # Extract core arXiv ID from the entry_id URL\n",
    "            arxiv_id = result.entry_id.split('/')[-1]\n",
    "            \n",
    "            # Extract author names into a simple list\n",
    "            author_names = [author.name for author in result.authors]\n",
    "            \n",
    "            # Create a dictionary with all the desired metadata\n",
    "            paper_info = {\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'title': result.title,\n",
    "                'authors': author_names,\n",
    "                'summary': result.summary,\n",
    "                'published_date': result.published,\n",
    "                'updated_date': result.updated,\n",
    "                'pdf_url': result.pdf_url,\n",
    "                'primary_category': result.primary_category,\n",
    "                'all_categories': result.categories,\n",
    "                'doi': result.doi,\n",
    "                'journal_ref': result.journal_ref,\n",
    "                'comments': result.comment\n",
    "            }\n",
    "            papers_data.append(paper_info)\n",
    "            \n",
    "    if not papers_data:\n",
    "        print(\"No new papers found in the specified categories for the last week.\")\n",
    "        return None\n",
    "        \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(papers_data)\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully collected metadata for {len(df)} papers.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b23a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Fetching metadata for the latest 100 AI papers from the last 7 days...\n",
      "\n",
      "âœ… Successfully collected metadata for 100 papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nm/pb4pcvl954j110mq4qv7wj080000gn/T/ipykernel_21185/4282447663.py:26: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "df = fetch_ai_papers_to_dataframe(days_ago=7, max_results=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f882b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>all_categories</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2510.05102v1</td>\n",
       "      <td>TopInG: Topologically Interpretable Graph Lear...</td>\n",
       "      <td>[Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin ...</td>\n",
       "      <td>Graph Neural Networks (GNNs) have shown remark...</td>\n",
       "      <td>2025-10-06 17:59:44+00:00</td>\n",
       "      <td>2025-10-06 17:59:44+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.05102v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CG, math.AT, stat.ML, 55N31,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>submitted to ICML 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2510.05097v1</td>\n",
       "      <td>Pulp Motion: Framing-aware multimodal camera a...</td>\n",
       "      <td>[Robin Courant, Xi Wang, David Loiseaux, Marc ...</td>\n",
       "      <td>Treating human motion and camera trajectory ge...</td>\n",
       "      <td>2025-10-06 17:58:34+00:00</td>\n",
       "      <td>2025-10-06 17:58:34+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.05097v1</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>[cs.GR, cs.CV]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Project page:\\n  https://www.lix.polytechnique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2510.05096v1</td>\n",
       "      <td>Paper2Video: Automatic Video Generation from S...</td>\n",
       "      <td>[Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou]</td>\n",
       "      <td>Academic presentation videos have become an es...</td>\n",
       "      <td>2025-10-06 17:58:02+00:00</td>\n",
       "      <td>2025-10-06 17:58:02+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.05096v1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, cs.AI, cs.CL, cs.MA, cs.MM]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20 pages, 8 figures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2510.05095v1</td>\n",
       "      <td>From Noisy Traces to Stable Gradients: Bias-Va...</td>\n",
       "      <td>[Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zha...</td>\n",
       "      <td>Large reasoning models (LRMs) generate interme...</td>\n",
       "      <td>2025-10-06 17:58:01+00:00</td>\n",
       "      <td>2025-10-06 17:58:01+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.05095v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CL]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2510.05094v1</td>\n",
       "      <td>VChain: Chain-of-Visual-Thought for Reasoning ...</td>\n",
       "      <td>[Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu,...</td>\n",
       "      <td>Recent video generation models can produce smo...</td>\n",
       "      <td>2025-10-06 17:57:59+00:00</td>\n",
       "      <td>2025-10-06 17:57:59+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.05094v1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Project page: https://eyeline-labs.github.io/V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2510.04862v1</td>\n",
       "      <td>Video Game Level Design as a Multi-Agent Reinf...</td>\n",
       "      <td>[Sam Earle, Zehua Jiang, Eugene Vinitsky, Juli...</td>\n",
       "      <td>Procedural Content Generation via Reinforcemen...</td>\n",
       "      <td>2025-10-06 14:49:21+00:00</td>\n",
       "      <td>2025-10-06 14:49:21+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.04862v1</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>[cs.AI, cs.LG, cs.MA, cs.NE]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 pages, 7 tables, 5 figures, published as fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2510.04861v1</td>\n",
       "      <td>A Clinical-grade Universal Foundation Model fo...</td>\n",
       "      <td>[Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing C...</td>\n",
       "      <td>Intraoperative pathology is pivotal to precisi...</td>\n",
       "      <td>2025-10-06 14:48:43+00:00</td>\n",
       "      <td>2025-10-06 14:48:43+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.04861v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2510.04860v1</td>\n",
       "      <td>Alignment Tipping Process: How Self-Evolution ...</td>\n",
       "      <td>[Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan,...</td>\n",
       "      <td>As Large Language Model (LLM) agents increasin...</td>\n",
       "      <td>2025-10-06 14:48:39+00:00</td>\n",
       "      <td>2025-10-06 14:48:39+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.04860v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2510.04859v1</td>\n",
       "      <td>Î¼DeepIQA: deep learning-based fast and robust ...</td>\n",
       "      <td>[Elena Corbetta, Thomas Bocklitz]</td>\n",
       "      <td>Optical microscopy is one of the most widely u...</td>\n",
       "      <td>2025-10-06 14:48:36+00:00</td>\n",
       "      <td>2025-10-06 14:48:36+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.04859v1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, physics.data-an, q-bio.QM]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>16 pages, 6 figures. \\mu DeepIQA is publicly a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2510.04856v1</td>\n",
       "      <td>ERDE: Entropy-Regularized Distillation for Ear...</td>\n",
       "      <td>[Martial Guidez, Stefan Duffner, Yannick Alpou...</td>\n",
       "      <td>Although deep neural networks and in particula...</td>\n",
       "      <td>2025-10-06 14:45:41+00:00</td>\n",
       "      <td>2025-10-06 14:45:41+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2510.04856v1</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        arxiv_id                                              title  \\\n",
       "0   2510.05102v1  TopInG: Topologically Interpretable Graph Lear...   \n",
       "1   2510.05097v1  Pulp Motion: Framing-aware multimodal camera a...   \n",
       "2   2510.05096v1  Paper2Video: Automatic Video Generation from S...   \n",
       "3   2510.05095v1  From Noisy Traces to Stable Gradients: Bias-Va...   \n",
       "4   2510.05094v1  VChain: Chain-of-Visual-Thought for Reasoning ...   \n",
       "..           ...                                                ...   \n",
       "95  2510.04862v1  Video Game Level Design as a Multi-Agent Reinf...   \n",
       "96  2510.04861v1  A Clinical-grade Universal Foundation Model fo...   \n",
       "97  2510.04860v1  Alignment Tipping Process: How Self-Evolution ...   \n",
       "98  2510.04859v1  Î¼DeepIQA: deep learning-based fast and robust ...   \n",
       "99  2510.04856v1  ERDE: Entropy-Regularized Distillation for Ear...   \n",
       "\n",
       "                                              authors  \\\n",
       "0   [Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin ...   \n",
       "1   [Robin Courant, Xi Wang, David Loiseaux, Marc ...   \n",
       "2     [Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou]   \n",
       "3   [Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zha...   \n",
       "4   [Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu,...   \n",
       "..                                                ...   \n",
       "95  [Sam Earle, Zehua Jiang, Eugene Vinitsky, Juli...   \n",
       "96  [Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing C...   \n",
       "97  [Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan,...   \n",
       "98                  [Elena Corbetta, Thomas Bocklitz]   \n",
       "99  [Martial Guidez, Stefan Duffner, Yannick Alpou...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Graph Neural Networks (GNNs) have shown remark...   \n",
       "1   Treating human motion and camera trajectory ge...   \n",
       "2   Academic presentation videos have become an es...   \n",
       "3   Large reasoning models (LRMs) generate interme...   \n",
       "4   Recent video generation models can produce smo...   \n",
       "..                                                ...   \n",
       "95  Procedural Content Generation via Reinforcemen...   \n",
       "96  Intraoperative pathology is pivotal to precisi...   \n",
       "97  As Large Language Model (LLM) agents increasin...   \n",
       "98  Optical microscopy is one of the most widely u...   \n",
       "99  Although deep neural networks and in particula...   \n",
       "\n",
       "              published_date              updated_date  \\\n",
       "0  2025-10-06 17:59:44+00:00 2025-10-06 17:59:44+00:00   \n",
       "1  2025-10-06 17:58:34+00:00 2025-10-06 17:58:34+00:00   \n",
       "2  2025-10-06 17:58:02+00:00 2025-10-06 17:58:02+00:00   \n",
       "3  2025-10-06 17:58:01+00:00 2025-10-06 17:58:01+00:00   \n",
       "4  2025-10-06 17:57:59+00:00 2025-10-06 17:57:59+00:00   \n",
       "..                       ...                       ...   \n",
       "95 2025-10-06 14:49:21+00:00 2025-10-06 14:49:21+00:00   \n",
       "96 2025-10-06 14:48:43+00:00 2025-10-06 14:48:43+00:00   \n",
       "97 2025-10-06 14:48:39+00:00 2025-10-06 14:48:39+00:00   \n",
       "98 2025-10-06 14:48:36+00:00 2025-10-06 14:48:36+00:00   \n",
       "99 2025-10-06 14:45:41+00:00 2025-10-06 14:45:41+00:00   \n",
       "\n",
       "                              pdf_url primary_category  \\\n",
       "0   http://arxiv.org/pdf/2510.05102v1            cs.LG   \n",
       "1   http://arxiv.org/pdf/2510.05097v1            cs.GR   \n",
       "2   http://arxiv.org/pdf/2510.05096v1            cs.CV   \n",
       "3   http://arxiv.org/pdf/2510.05095v1            cs.LG   \n",
       "4   http://arxiv.org/pdf/2510.05094v1            cs.CV   \n",
       "..                                ...              ...   \n",
       "95  http://arxiv.org/pdf/2510.04862v1            cs.AI   \n",
       "96  http://arxiv.org/pdf/2510.04861v1            cs.LG   \n",
       "97  http://arxiv.org/pdf/2510.04860v1            cs.LG   \n",
       "98  http://arxiv.org/pdf/2510.04859v1            cs.CV   \n",
       "99  http://arxiv.org/pdf/2510.04856v1            cs.CV   \n",
       "\n",
       "                                       all_categories   doi journal_ref  \\\n",
       "0   [cs.LG, cs.AI, cs.CG, math.AT, stat.ML, 55N31,...  None        None   \n",
       "1                                      [cs.GR, cs.CV]  None        None   \n",
       "2                 [cs.CV, cs.AI, cs.CL, cs.MA, cs.MM]  None        None   \n",
       "3                               [cs.LG, cs.AI, cs.CL]  None        None   \n",
       "4                                             [cs.CV]  None        None   \n",
       "..                                                ...   ...         ...   \n",
       "95                       [cs.AI, cs.LG, cs.MA, cs.NE]  None        None   \n",
       "96                                            [cs.LG]  None        None   \n",
       "97                                     [cs.LG, cs.AI]  None        None   \n",
       "98                 [cs.CV, physics.data-an, q-bio.QM]  None        None   \n",
       "99                                     [cs.CV, cs.LG]  None        None   \n",
       "\n",
       "                                             comments  \n",
       "0                              submitted to ICML 2025  \n",
       "1   Project page:\\n  https://www.lix.polytechnique...  \n",
       "2                                 20 pages, 8 figures  \n",
       "3                                                None  \n",
       "4   Project page: https://eyeline-labs.github.io/V...  \n",
       "..                                                ...  \n",
       "95  11 pages, 7 tables, 5 figures, published as fu...  \n",
       "96                                               None  \n",
       "97                                               None  \n",
       "98  16 pages, 6 figures. \\mu DeepIQA is publicly a...  \n",
       "99                                               None  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
